Assumptions:
a) This is essentially a poor man's url shortener. There is a no disk-based data persistence; however, as I would imagine, we need this to be production ready.
b) API is access pattern. Provide full URL and fetch short url. Provide short URL(without domain) and fetch full URL.
c) No UI.

Scale issue:
First, we need to determine whether the service is network bound, CPU bound, and/or I/O bound, and the max throughput obtained per server.
Encoding/decoding process will be a CPU bound operation; however, the data persistence is going to be I/O bound.

To achieve horizontal scalability, we need to change our service such that the following components can be individually scaled:

- Application server;
- Storage layer;
- Caching layer;
- Unique Id generation.

- Application server
We need multiple application servers to increase our capacity beyond a single server. These application servers will sit behind a load balancer such as HAProxy (with redundancy). 
Having a load balancer will also help us to redirect traffic to different application servers (one that handles shortening and the other that handles redirects) if we find that redirects are heavily used as compared to shortening service.

- Storage layer
Since the access pattern is very straightforward, we need a good distributed key value store that provides persistence, redundancy, along with horizontal scalability - Eg. Cassandra/DynamoDB/CouchBase/Riak.

- Caching layer
Standard stuff - Memcached with McRouter flavor/Redis

- Unique Id generation
This should be treated as a separate service so that it can be scaled independently.
Couple of options here:-
a) Zookeeper provides distributed counters; however, its performance needs to be measured;
b) Twitter's network Id generation service -- Snowflake - 64 bit IDs that are sortable (first 48-bits are timestamp). 
In case of ID collision (will be rare)-- regenerate the ID.


Data retention policy:
----------------------
We cannot store data indefinitely. We can use our sortable unique IDs to purge data from KV store and caches.
Note - we can either decide to purge or reuse the IDs




